{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.11.0\n",
      "TFX version: 1.12.0\n",
      "MLMD version: 1.12.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import ml_metadata as mlmd\n",
    "from tfx import v1 as tfx\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "import pipeline_settings as settings\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "print('TF version: {}'.format(tf.__version__))\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "print('MLMD version: {}'.format(mlmd.__version__))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_context = InteractiveContext(\n",
    "    pipeline_name=settings.PIPELINE_NAME,\n",
    "    pipeline_root=settings.PIPELINE_ROOT,\n",
    "    metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(\n",
    "        settings.METADATA_PATH)\n",
    ")\n",
    "\n",
    "connection_config = interactive_context.metadata_connection_config\n",
    "store = mlmd.MetadataStore(connection_config)\n",
    "\n",
    "# All TFX artifacts are stored in the base directory\n",
    "base_dir = connection_config.sqlite.filename_uri.split('metadata.sqlite')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_types(types):\n",
    "  # Helper function to render dataframes for the artifact and execution types\n",
    "  table = {'id': [], 'name': []}\n",
    "  for a_type in types:\n",
    "    table['id'].append(a_type.id)\n",
    "    table['name'].append(a_type.name)\n",
    "  return pd.DataFrame(data=table)\n",
    "\n",
    "\n",
    "def display_artifacts(store, artifacts):\n",
    "  # Helper function to render dataframes for the input artifacts\n",
    "  table = {'artifact id': [], 'type': [], 'uri': []}\n",
    "  for a in artifacts:\n",
    "    table['artifact id'].append(a.id)\n",
    "    artifact_type = store.get_artifact_types_by_id([a.type_id])[0]\n",
    "    table['type'].append(artifact_type.name)\n",
    "    table['uri'].append(a.uri.replace(base_dir, './'))\n",
    "  return pd.DataFrame(data=table)\n",
    "\n",
    "\n",
    "def display_properties(store, node):\n",
    "  # Helper function to render dataframes for artifact and execution properties\n",
    "  table = {'property': [], 'value': []}\n",
    "  for k, v in node.properties.items():\n",
    "    table['property'].append(k)\n",
    "    table['value'].append(\n",
    "        v.string_value if v.HasField('string_value') else v.int_value)\n",
    "  for k, v in node.custom_properties.items():\n",
    "    table['property'].append(k)\n",
    "    table['value'].append(\n",
    "        v.string_value if v.HasField('string_value') else v.int_value)\n",
    "  return pd.DataFrame(data=table)\n",
    "\n",
    "\n",
    "def get_one_hop_parent_artifacts(store, artifacts):\n",
    "  # Get a list of artifacts within a 1-hop of the artifacts of interest\n",
    "  artifact_ids = [artifact.id for artifact in artifacts]\n",
    "  executions_ids = set(\n",
    "      event.execution_id\n",
    "      for event in store.get_events_by_artifact_ids(artifact_ids)\n",
    "      if event.type == mlmd.proto.Event.OUTPUT)\n",
    "  artifacts_ids = set(\n",
    "      event.artifact_id\n",
    "      for event in store.get_events_by_execution_ids(executions_ids)\n",
    "      if event.type == mlmd.proto.Event.INPUT)\n",
    "  return [artifact for artifact in store.get_artifacts_by_id(artifacts_ids)]\n",
    "\n",
    "\n",
    "def find_producer_execution(store, artifact):\n",
    "  executions_ids = set(\n",
    "      event.execution_id for event in store.get_events_by_artifact_ids([artifact.id]) if event.type == mlmd.proto.Event.OUTPUT\n",
    "    )\n",
    "  return store.get_executions_by_id(executions_ids)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_types(store.get_artifact_types())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_statistics_set = store.get_artifacts_by_type(\"ExampleStatistics\")\n",
    "display_artifacts(store, example_statistics_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_statistics = example_statistics_set[-1]\n",
    "display_properties(store, example_statistics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_artifacts = get_one_hop_parent_artifacts(store, [example_statistics])\n",
    "display_artifacts(store, parent_artifacts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_dataset = parent_artifacts[0]\n",
    "display_properties(store, exported_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_types(store.get_execution_types())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = find_producer_execution(store, exported_dataset)\n",
    "display_properties(store, trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration.experimental.interactive import standard_visualizations\n",
    "from tfx.orchestration.experimental.interactive import visualizations\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "# Non-public APIs, just for showcase.\n",
    "from tfx.orchestration.portable.mlmd import execution_lib\n",
    "\n",
    "# TODO(b/171447278): Move these functions into the TFX library.\n",
    "\n",
    "\n",
    "def get_latest_artifacts(metadata, pipeline_name, component_id):\n",
    "  \"\"\"Output artifacts of the latest run of the component.\"\"\"\n",
    "  context = metadata.store.get_context_by_type_and_name(\n",
    "      'node', f'{pipeline_name}.{component_id}')\n",
    "  executions = metadata.store.get_executions_by_context(context.id)\n",
    "  latest_execution = max(executions,\n",
    "                         key=lambda e: e.last_update_time_since_epoch)\n",
    "  return execution_lib.get_output_artifacts(metadata, latest_execution.id)\n",
    "\n",
    "\n",
    "# Non-public APIs, just for showcase.\n",
    "\n",
    "\n",
    "def visualize_artifacts(artifacts):\n",
    "  \"\"\"Visualizes artifacts using standard visualization modules.\"\"\"\n",
    "  for artifact in artifacts:\n",
    "    visualization = visualizations.get_registry().get_visualization(\n",
    "        artifact.type_name)\n",
    "    if visualization:\n",
    "      visualization.display(artifact)\n",
    "\n",
    "\n",
    "standard_visualizations.register_standard_visualizations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "latest_statistics = store.get_artifacts_by_type(\"ExampleStatistics\")[-1]\n",
    "\n",
    "stats_uri = latest_statistics.uri + '/FeatureStats.pb'\n",
    "stats = tfdv.load_stats_binary(stats_uri)\n",
    "\n",
    "lateast_schema = store.get_artifacts_by_type(\"Schema\")[-1]\n",
    "schema_uri = lateast_schema.uri + '/schema.pbtxt'\n",
    "schema = tfdv.load_schema_text(schema_uri)\n",
    "\n",
    "tfdv.validate_statistics(stats, schema)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_eval_uri = latest_statistics.uri + '/FeatureStats.pb'\n",
    "stats_eval = tfdv.load_stats_binary(stats_eval_uri)\n",
    "tfdv.visualize_statistics(stats, stats_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = store.get_artifacts_by_type(\"Examples\")\n",
    "display_artifacts(store, examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_uri = examples[-1].uri + \\\n",
    "    '/Split-train/transformed_examples-00000-of-00001.gz'\n",
    "tf_dataset = tf.data.TFRecordDataset(dataset_uri, compression_type='GZIP')\n",
    "lengt_dataset = tf_dataset.reduce(0, lambda x,_: x+1).numpy()\n",
    "\n",
    "print(f'Number of train examples: {lengt_dataset}')\n",
    "dataset_eval_uri = examples[-1].uri + \\\n",
    "    '/Split-eval/transformed_examples-00000-of-00001.gz'\n",
    "tf_dataset_eval = tf.data.TFRecordDataset(\n",
    "    dataset_eval_uri, compression_type='GZIP')\n",
    "lengt_dataset_eval = tf_dataset_eval.reduce(0, lambda x, _: x+1).numpy()\n",
    "print(f'Number of validation examples: {lengt_dataset_eval}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "feature_spec = schema_utils.schema_as_feature_spec(schema).feature_spec\n",
    "print(feature_spec)\n",
    "\n",
    "\n",
    "# Create a batch from the dataset\n",
    "for records in tf_dataset.batch(1).take(1):\n",
    "\n",
    "  # Parse the batch to get a dictionary of raw features\n",
    "  parsed_examples = tf.io.parse_example(records, feature_spec)\n",
    "  \n",
    "  record = records[0]\n",
    "  image = parsed_examples[\"image\"][0]\n",
    "  plt.imshow(image, interpolation='nearest', cmap='gray')\n",
    "  plt.show()\n",
    "\n",
    "  # Print the results\n",
    "  print(\"\\nRAW FEATURES:\")\n",
    "  for key, value in parsed_examples.items():\n",
    "    print(f'{key}: {value.numpy()}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to parse the `tf.train.Example` protocol buffer\n",
    "# def parse_fn(example):\n",
    "#     features = {\n",
    "#         'image': tf.io.FixedLenFeature([], tf.string),\n",
    "#         'label': tf.io.FixedLenFeature([], tf.int64)\n",
    "#     }\n",
    "#     parsed_example = tf.io.parse_single_example(example, features)\n",
    "#     print(example)\n",
    "#     print(parsed_example)\n",
    "#     return parsed_example\n",
    "\n",
    "\n",
    "# # Apply the parse function to the dataset\n",
    "# dataset_inspect = tf_dataset.map(parse_fn)\n",
    "\n",
    "# # one_example = next(iter(tf_dataset.take(1)))\n",
    "# # parsed_example = parse_fn(one_example)\n",
    "# # numpy_image = tf.io.parse_tensor(parsed_example[\"image\"], out_type=tf.uint8)\n",
    "\n",
    "# # Iterate over the dataset and print the features of each example\n",
    "# for example in dataset_inspect.take(1):\n",
    "#     numpy_image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
    "#     print(f'Image shape: {numpy_image.shape}')\n",
    "#     plt.imshow(numpy_image, interpolation='nearest', cmap='gray')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_debug = store.get_artifacts_by_type(\"Examples\")\n",
    "dataset_debug_uri = examples[-1].uri\n",
    "import_examples = tfx.components.ImportExampleGen(dataset_debug_uri)\n",
    "#run the component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_debug = store.get_artifacts_by_type(\"Examples\")\n",
    "dataset_debug_uri = examples[-1].uri + \\\n",
    "    '/Split-train/transformed_examples-00000-of-00001.gz'\n",
    "\n",
    "tf_dataset = tf.data.TFRecordDataset(dataset_uri, compression_type='GZIP')\n",
    "lengt_dataset = tf_dataset.reduce(0, lambda x, _: x+1).numpy()\n",
    "\n",
    "print(f'Number of train examples: {lengt_dataset}')\n",
    "dataset_eval_uri = examples[-1].uri + \\\n",
    "    '/Split-eval/transformed_examples-00000-of-00001.gz'\n",
    "tf_dataset_eval = tf.data.TFRecordDataset(\n",
    "    dataset_eval_uri, compression_type='GZIP')\n",
    "lengt_dataset_eval = tf_dataset_eval.reduce(0, lambda x, _: x+1).numpy()\n",
    "print(f'Number of validation examples: {lengt_dataset_eval}')\n",
    "\n",
    "dataset_importer = tfx.dsl.Importer(\n",
    "    source_uri=dataset_uri,\n",
    "    artifact_type=tfx.types.standard_artifacts.Examples).with_id(\n",
    "        'dataset_importer')\n",
    "\n",
    "lateast_schema = store.get_artifacts_by_type(\"Schema\")[-1]\n",
    "schema__debug_uri = lateast_schema.uri + '/schema.pbtxt'\n",
    "\n",
    "schema_importer = tfx.dsl.Importer(\n",
    "    source_uri=schema_uri,\n",
    "    artifact_type=tfx.types.standard_artifacts.Schema).with_id(\n",
    "    'schema_importer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluations = store.get_artifacts_by_type(\"ModelEvaluation\")\n",
    "display_artifacts(store, model_evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "model_evaluation = model_evaluations[-1]\n",
    "evaluation_uri = model_evaluation.uri\n",
    "print(evaluation_uri)\n",
    "eval_result = tfma.load_eval_result(evaluation_uri)\n",
    "# tfma.view.render_slicing_metrics(eval_result, slicing_column='label')\n",
    "eval_result.slicing_metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
